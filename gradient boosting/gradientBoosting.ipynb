{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "Gradient Boosting là thuật toán học có giám sát thuộc lớp ensemble. Nó kết hợp nhiều model yếu hơn để tạo 1 model mạnh. Thuật toán đệ quy bổ sung thêm các cây vào model với target là pseudo residual và điều chỉnh weight theo gradient của lost function. Mô hình cuối cùng là một weighted sum của các cây trong mô hình. Gradient Boosting là phương pháp rất hiệu quả trong các bàn toán thực tế. \n",
    "## 2. Math Foundation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ta có mô hình tổng quát là weighted sum của các mô hình yếu hơn từ lớp H\n",
    "$$\\hat{F} = \\sum_{m=1}^{M}\\gamma_m h_m(x) + const$$\n",
    "Ta approximate $\\hat{F}$ bằng cách tìm xấp xỉ các parameters model cho hàm loss function đạt cực tiểu. Ta xấp xỉ theo phương pháp tham lam, tức là ta bắt đầu từ tìm cực tiểu hàm mất mát cho mô hình base, tiếp tục mở rộng làm loss và làm tương tự.\n",
    "$$F_0(x) = \\argmin_{h_m \\in H} \\sum_{i=1}^nL(y_i, h_m(x_i))$$\n",
    "$$F_m(x) = F_{m-1}x + \\argmin_{h_m \\in H} \\sum_{i=1}^nL(y_i, F_{m-1}x_i + h_m(x_i))$$\n",
    "Rất khó để giải nghiệm đúng của phương trình này, ta sẽ dùng phương pháp gradient descent để xấp xi nghiệm\n",
    "$$F_m(x) = F_{m-1}x-\\gamma \\sum_{i=1}^n \\nabla F_{m-1}L(y_i, F_{m-1}x_i)$$\n",
    "với $\\gamma > 0$ \n",
    "\n",
    "\n",
    "**Optimal value for $\\gamma$** \n",
    "$$\\gamma_m = \\argmin_\\gamma \\sum_{i=1}^nL(y_i, F_mx_i)= \\argmin_\\gamma L(y_i, F_{m-1} - \\gamma \\nabla F_{m-1}L(y_i, F_{m-1}x_i))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pseudo code\n",
    "1. Initialize model with constant value: $F_0x = \\argmin_\\gamma \\sum_{i=1}^nL(y_i, \\gamma)$\n",
    "2. For m = 1 to M:\n",
    "- Compute pseudo-residuals: \n",
    "$$r_im = - \\left[ \\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)} \\right]_{F(x_i) = F_{m-1}(x_i)} \\quad \\text{for } i = 1, \\dots, n.\n",
    "$$\n",
    "- Fit a base learner under scaling $h_m(x)$ to pseudo residual\n",
    "- Compute $\\gamma_m$ by solving the optimization problem\n",
    "$$\\gamma_m = \\argmin_\\gamma \\sum_{i-1}^nL(y_i, F_{m-1}x_i+\\gamma h_m(x_i))$$\n",
    "- Update model \n",
    "$$F_m(x) = F_{m-1}(x) + \\gamma_m h_m(x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implementation\n",
    "Tai mỗi node, ta cần tìm một giá trị constant gamma để gán cho giá trị đầu ra của node\n",
    "Giá trị gamma này cần phải tối ưu, theo nghĩa làm giảm error nhiều nhất có thể.\n",
    "$$L = \\sum(y_i - f(x_i))^2 = \\sum(y_i = f_current(x_i) + \\gamma) $$\n",
    "Lấy gradient và giải cho ptr gradient = 0 ta có\n",
    "$$\\frac{\\partial L}{\\partial y} = -2 \\sum(y_i -f_{current}(x_i) - \\gamma) = 0 \\iff \\sum(y_i - f_{current}(x_i)) = \\sum(\\gamma) $$\n",
    "$$ \\iff \\gamma = \\frac{y_i-f_{current}x_i}{n}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientBoosting():\n",
    "    def __init__(self, n_estimators = 100, lamda = 1.0):\n",
    "        \"\"\"\n",
    "        parameters: \n",
    "        n_estimators (int): number of base model, default 100\n",
    "        lamda (float): learning rate, default 1.0 \n",
    "        \"\"\"    \n",
    "        self.n_estimators = n_estimators\n",
    "        self.lamda = lamda\n",
    "        self.trees = []\n",
    "    def __initialise(self, y):\n",
    "        n_observations = len(y)\n",
    "        self.init_gamma = np.mean(y)\n",
    "        f0 = np.full((n_observations, ), self.init_gamma)\n",
    "        return f0\n",
    "    def _compute_gamma(self, residuals, leaf_ids):\n",
    "        \"\"\"\n",
    "        Compute optimal gamma\n",
    "        Parameters:\n",
    "        residuals: error\n",
    "        leaf_ids: indices of leaves corresponding observation\n",
    "        return:\n",
    "        gamma: the optimal value for gamma\n",
    "        \"\"\"\n",
    "        leaves = np.unique(leaf_ids)\n",
    "        gamma = np.zeros(residuals.shape)\n",
    "        for leaf in leaves: \n",
    "            ids = np.where(leaf_ids == leaf)[0]\n",
    "            gamma[ids] = np.mean(residuals[ids])\n",
    "        return gamma\n",
    "    def fit(self, X, y):\n",
    "        #initialise base model\n",
    "        self.f0 = self.__initialise(y)\n",
    "        #assign current f by f0\n",
    "        f_current = self.f0\n",
    "        #loop M timesL\n",
    "        for i in range(self.n_estimators):\n",
    "            #calculate residual\n",
    "            residual = y - f_current\n",
    "            #fit a tree to residual\n",
    "            tree = DecisionTreeRegressor()\n",
    "            tree.fit(X, residual)\n",
    "            leaf_ids = tree.apply(X)\n",
    "            gamma = self._compute_gamma(residual, leaf_ids)\n",
    "            f_current = f_current + self.lamda*gamma\n",
    "            self.trees.append(tree)\n",
    "    def predict(self, X):\n",
    "        size = X.shape[0]\n",
    "        pred = np.full((size, ), self.init_gamma)\n",
    "        for tree in self.trees:\n",
    "            residual = tree.predict(X)\n",
    "            leaf_ids = tree.apply(X)\n",
    "            gamma = self._compute_gamma(residual, leaf_ids)\n",
    "            pred = pred + self.lamda*gamma\n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = datasets.load_diabetes()\n",
    "X, y = df.data, df.target\n",
    "model = GradientBoosting(n_estimators=100, lamda=0.1)\n",
    "model.fit(X, y)\n",
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.183580705289566e-06"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Thư viện sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.80808773702266"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "model_2 = GradientBoostingRegressor(max_depth=6, n_estimators=100, learning_rate=0.1)\n",
    "model_2.fit(X, y)\n",
    "y_pred_2 = model_2.predict(X)\n",
    "mean_squared_error(y_pred_2, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Pros and cons \n",
    "**Pros**\n",
    "- Flexibility: có thể sử dụng cho nhiều bài toán: phân loại, regression, ranking\n",
    "- Can handle non linear relationship: Gradient boosting có thể thể hiện mối quan hệ phức tạp giữa biến đầu vào và biến đầu ra bằng cách kết hợp nhiều decision tree với weight khác nhau\n",
    "- High accuracy: được đánh giá là có độ chính xác cao và thường được sử dụng trogn nhiều cuộc thi\n",
    "- Feature selection: naturally identifies and prioritise the most important features\n",
    "- Reduction to bias: by sequential error correction and region - specific adjustment. Areas with high bias receive more attention in subsequent iterations. Each tree divides the feature space into regions (leaves) and provides specific corrections for each region\n",
    "This allows the model to learn different adjustments for different parts of the feature space.\n",
    "- Robustness to missing data\n",
    "- Relatively few assumption: does not require feature scaleing, no assumption about distribution of features or target\n",
    "-Handles unbalanced data: can be wieghted to addess class imbalance issues\n",
    "\n",
    "\n",
    "**Cons** \n",
    "- Overfitting risk\n",
    "- Computational intensity: can not be parallelised, training can be low\n",
    "- Memory requirement: entrire ensemple must be stored\n",
    "- Sensitive to outliers: the algorithm focus on correcting errors, which can cause it to overfit\n",
    "- Hyperparameter tuning complexity: require carefule tuning of multiple parameters( number of trees, tree depth, learning rate)\n",
    "- Limited Interpretability: full ensemple becomes a blackbox\n",
    "- Poor performance with sparse features: may not work for very sparse high dimensional data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
